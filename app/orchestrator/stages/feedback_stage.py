import logging
from typing import Dict, Any, Optional
from app.orchestrator.context import ExecutionContext
from app.adapters.runner.bedrock_runner import BedrockRunner
from app.core.schemas import PromptType

logger = logging.getLogger(__name__)

class FeedbackStage:
    """í”„ë¡¬í”„íŠ¸ ê°œì„  í”¼ë“œë°± ìƒì„± ë‹¨ê³„"""
    
    def __init__(self, context: ExecutionContext):
        self.context = context
        self.runner = BedrockRunner()
        # í”¼ë“œë°± ìƒì„±ìš© ëª¨ë¸ (ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©)
        self.feedback_model = "anthropic.claude-3-haiku-20240307-v1:0"
    
    async def execute(
        self, 
        evaluation_result: Dict[str, Any],
        prompt: str = "",
        prompt_type: PromptType = PromptType.TYPE_A,
        example_inputs: list = None
    ) -> Dict[str, Any]:
        """
        í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„  í”¼ë“œë°± ìƒì„±
        
        Args:
            evaluation_result: í‰ê°€ ê²°ê³¼ (ê° ì§€í‘œ ì ìˆ˜ + details)
            prompt: í‰ê°€ëœ í”„ë¡¬í”„íŠ¸
            prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì…
            example_inputs: ì˜ˆì‹œ ì…ë ¥ë“¤
            
        Returns:
            í”¼ë“œë°± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info("Generating prompt improvement feedback")
        
        if example_inputs is None:
            example_inputs = []
        
        try:
            # 1. í‰ê°€ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
            metrics = self._extract_metrics(evaluation_result)
            outputs = self._extract_outputs(evaluation_result)
            
            # 2. LLMì—ê²Œ í”¼ë“œë°± ìš”ì²­
            feedback_prompt = self._build_feedback_prompt(
                prompt=prompt,
                prompt_type=prompt_type,
                example_inputs=example_inputs,
                outputs=outputs,
                metrics=metrics
            )
            
            response = await self.runner.invoke(
                model=self.feedback_model,
                prompt=feedback_prompt,
                max_tokens=2000,
                temperature=0.3
            )
            
            # 3. ì‘ë‹µ íŒŒì‹±
            feedback = self._parse_feedback_response(response['output'], metrics)
            
            logger.info("Feedback generation completed")
            return feedback
            
        except Exception as e:
            logger.error(f"Feedback generation failed: {str(e)}")
            return self._generate_fallback_feedback(evaluation_result)
    
    def _extract_metrics(self, evaluation_result: Dict[str, Any]) -> Dict[str, float]:
        """í‰ê°€ ê²°ê³¼ì—ì„œ ì§€í‘œ ì ìˆ˜ ì¶”ì¶œ"""
        metrics = {}
        
        metric_keys = [
            'token_usage', 'information_density', 'consistency',
            'model_variance', 'hallucination', 'relevance'
        ]
        
        for key in metric_keys:
            if key in evaluation_result and evaluation_result[key]:
                score = evaluation_result[key].get('score', 0)
                metrics[key] = score
        
        return metrics
    
    def _extract_outputs(self, evaluation_result: Dict[str, Any]) -> list:
        """í‰ê°€ ê²°ê³¼ì—ì„œ ì¶œë ¥ ìƒ˜í”Œ ì¶”ì¶œ"""
        outputs = []
        
        if 'execution_results' in evaluation_result:
            exec_results = evaluation_result['execution_results']
            if 'executions' in exec_results:
                for exec_data in exec_results['executions'][:3]:  # ìµœëŒ€ 3ê°œë§Œ
                    if 'outputs' in exec_data and exec_data['outputs']:
                        # ì²« ë²ˆì§¸ ì¶œë ¥ë§Œ (ë„ˆë¬´ ê¸¸ë©´ ìë¦„)
                        output = exec_data['outputs'][0]
                        if len(output) > 500:
                            output = output[:500] + "..."
                        outputs.append(output)
        
        return outputs
    
    def _build_feedback_prompt(
        self,
        prompt: str,
        prompt_type: PromptType,
        example_inputs: list,
        outputs: list,
        metrics: Dict[str, float]
    ) -> str:
        """í”¼ë“œë°± ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì§€í‘œ ì„¤ëª…
        metric_descriptions = {
            'token_usage': 'í† í° ì‚¬ìš©ëŸ‰ (ë‚®ì„ìˆ˜ë¡ íš¨ìœ¨ì )',
            'information_density': 'ì •ë³´ ë°€ë„ (ë†’ì„ìˆ˜ë¡ ë°˜ë³µ ì ìŒ)',
            'consistency': 'ì¼ê´€ì„± (ë†’ì„ìˆ˜ë¡ ì¶œë ¥ì´ ì¼ì •)',
            'model_variance': 'ëª¨ë¸ í¸ì°¨ (ë†’ì„ìˆ˜ë¡ ëª¨ë¸ ê°„ ì°¨ì´ ì ìŒ)',
            'hallucination': 'í™˜ê° íƒì§€ (ë†’ì„ìˆ˜ë¡ ì‚¬ì‹¤ ì •í™•)',
            'relevance': 'ê´€ë ¨ì„± (ë†’ì„ìˆ˜ë¡ ì…ë ¥-ì¶œë ¥ ì—°ê´€ì„± ë†’ìŒ)'
        }
        
        # ì§€í‘œ ì ìˆ˜ ë¬¸ìì—´ ìƒì„±
        metrics_str = "\n".join([
            f"- {metric_descriptions.get(k, k)}: {v:.1f}ì "
            for k, v in metrics.items()
        ])
        
        # ì˜ˆì‹œ ì…ë ¥ ë¬¸ìì—´
        inputs_str = "\n".join([
            f"- ì…ë ¥ {i+1}: {getattr(inp, 'content', str(inp))[:100]}"
            for i, inp in enumerate(example_inputs[:3])
        ])
        
        # ì¶œë ¥ ìƒ˜í”Œ ë¬¸ìì—´
        outputs_str = "\n".join([
            f"- ì¶œë ¥ {i+1}: {out[:200]}..." if len(out) > 200 else f"- ì¶œë ¥ {i+1}: {out}"
            for i, out in enumerate(outputs[:3])
        ])
        
        prompt_type_str = {
            PromptType.TYPE_A: "ì •ë³´/ì‚¬ì‹¤ ìš”êµ¬í˜•",
            PromptType.TYPE_B_TEXT: "ì°½ì‘ ê¸€ ìƒì„±í˜•",
            PromptType.TYPE_B_IMAGE: "ì´ë¯¸ì§€ ìƒì„±í˜•"
        }.get(prompt_type, "ì•Œ ìˆ˜ ì—†ìŒ")
        
        return f"""ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ í”„ë¡¬í”„íŠ¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³ , ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.

## í”„ë¡¬í”„íŠ¸ ì •ë³´
- íƒ€ì…: {prompt_type_str}
- í”„ë¡¬í”„íŠ¸: "{prompt}"

## ì˜ˆì‹œ ì…ë ¥
{inputs_str}

## ì¶œë ¥ ìƒ˜í”Œ
{outputs_str}

## í‰ê°€ ì ìˆ˜ (100ì  ë§Œì )
{metrics_str}

## ìš”ì²­ì‚¬í•­
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. ì „ì²´ ë¶„ì„ (2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë¬¸ì œì  ìš”ì•½)

2. ì§€í‘œë³„ ê°œì„  ì—¬ì§€ (ê° ì§€í‘œì— ëŒ€í•´ í•œ ì¤„ì”©):
- [ì§€í‘œëª…] ([í˜„ì¬ì ìˆ˜]ì ): [ê°œì„  ë°©ì•ˆ ë° ì˜ˆìƒ íš¨ê³¼]

3. ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì œì•ˆ (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ)

ì ìˆ˜ê°€ 80ì  ì´ìƒì¸ ì§€í‘œëŠ” "í˜„ì¬ ì–‘í˜¸" ì •ë„ë¡œ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ê³ ,
ì ìˆ˜ê°€ ë‚®ì€ ì§€í‘œì— ì§‘ì¤‘í•´ì„œ êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
    
    def _parse_feedback_response(self, response: str, metrics: Dict[str, float]) -> Dict[str, Any]:
        """LLM ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í”¼ë“œë°±ìœ¼ë¡œ íŒŒì‹±"""
        
        # ê¸°ë³¸ êµ¬ì¡°
        feedback = {
            'overall_analysis': '',
            'metric_improvements': [],
            'improved_prompt': '',
            'raw_feedback': response
        }
        
        lines = response.strip().split('\n')
        current_section = None
        section_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ì„¹ì…˜ ê°ì§€
            if 'ì „ì²´ ë¶„ì„' in line or 'í•µì‹¬ ë¬¸ì œì ' in line:
                if current_section and section_content:
                    self._save_section(feedback, current_section, section_content)
                current_section = 'overall'
                section_content = []
            elif 'ì§€í‘œë³„' in line or 'ê°œì„  ì—¬ì§€' in line:
                if current_section and section_content:
                    self._save_section(feedback, current_section, section_content)
                current_section = 'metrics'
                section_content = []
            elif 'ê°œì„ ëœ í”„ë¡¬í”„íŠ¸' in line or 'í”„ë¡¬í”„íŠ¸ ì œì•ˆ' in line:
                if current_section and section_content:
                    self._save_section(feedback, current_section, section_content)
                current_section = 'prompt'
                section_content = []
            elif current_section:
                section_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section and section_content:
            self._save_section(feedback, current_section, section_content)
        
        # ì§€í‘œë³„ ê°œì„  ì—¬ì§€ íŒŒì‹±
        if not feedback['metric_improvements']:
            feedback['metric_improvements'] = self._generate_metric_improvements(metrics)
        
        return feedback
    
    def _save_section(self, feedback: Dict, section: str, content: list):
        """ì„¹ì…˜ ë‚´ìš© ì €ì¥"""
        text = '\n'.join(content).strip()
        
        if section == 'overall':
            feedback['overall_analysis'] = text
        elif section == 'metrics':
            # ì§€í‘œë³„ ê°œì„  ì—¬ì§€ íŒŒì‹±
            improvements = []
            for line in content:
                if line.startswith('-') or line.startswith('â€¢'):
                    improvements.append(line.lstrip('-â€¢').strip())
            feedback['metric_improvements'] = improvements
        elif section == 'prompt':
            feedback['improved_prompt'] = text
    
    def _generate_metric_improvements(self, metrics: Dict[str, float]) -> list:
        """ì§€í‘œ ì ìˆ˜ ê¸°ë°˜ ê¸°ë³¸ ê°œì„  ì œì•ˆ ìƒì„±"""
        improvements = []
        
        suggestions = {
            'token_usage': {
                'low': 'ë¶ˆí•„ìš”í•œ ì§€ì‹œì‚¬í•­ ì œê±° ì‹œ íš¨ìœ¨ì„± ê°œì„  ê°€ëŠ¥',
                'high': 'í˜„ì¬ ì–‘í˜¸, í° ë³€í™” ì—†ì„ ê²ƒ'
            },
            'information_density': {
                'low': '"ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜" ì§€ì‹œ ì¶”ê°€ ì‹œ ê°œì„  ê°€ëŠ¥',
                'high': 'í˜„ì¬ ì–‘í˜¸, í° ë³€í™” ì—†ì„ ê²ƒ'
            },
            'consistency': {
                'low': 'ì¶œë ¥ í˜•ì‹ ëª…ì‹œ ì‹œ ìƒìŠ¹ ì—¬ì§€ ë†’ìŒ',
                'high': 'í˜„ì¬ ì–‘í˜¸, í° ë³€í™” ì—†ì„ ê²ƒ'
            },
            'model_variance': {
                'low': 'ì œì•½ì¡°ê±´ ëª…í™•í™” ì‹œ ìƒìŠ¹ ê¸°ëŒ€',
                'high': 'í˜„ì¬ ì–‘í˜¸, í° ë³€í™” ì—†ì„ ê²ƒ'
            },
            'hallucination': {
                'low': '"ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì¤˜" ë˜ëŠ” "í™•ì‹¤í•œ ì •ë³´ë§Œ" ì¶”ê°€ ì‹œ ê°œì„  ê°€ëŠ¥',
                'high': 'í˜„ì¬ ì–‘í˜¸, í° ë³€í™” ì—†ì„ ê²ƒ'
            },
            'relevance': {
                'low': 'ì§ˆë¬¸ ë²”ìœ„ë¥¼ ì¢íˆê±°ë‚˜ êµ¬ì²´í™” ì‹œ ê°œì„  ê°€ëŠ¥',
                'high': 'í˜„ì¬ ì–‘í˜¸, í° ë³€í™” ì—†ì„ ê²ƒ'
            }
        }
        
        metric_names = {
            'token_usage': 'í† í° ì‚¬ìš©ëŸ‰',
            'information_density': 'ì •ë³´ ë°€ë„',
            'consistency': 'ì¼ê´€ì„±',
            'model_variance': 'ëª¨ë¸ í¸ì°¨',
            'hallucination': 'í™˜ê° íƒì§€',
            'relevance': 'ê´€ë ¨ì„±'
        }
        
        for metric, score in metrics.items():
            if metric in suggestions:
                name = metric_names.get(metric, metric)
                level = 'high' if score >= 80 else 'low'
                suggestion = suggestions[metric][level]
                improvements.append(f"{name} ({score:.0f}ì ): {suggestion}")
        
        return improvements
    
    def _generate_fallback_feedback(self, evaluation_result: Dict[str, Any]) -> Dict[str, Any]:
        """LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”¼ë“œë°± ìƒì„±"""
        metrics = self._extract_metrics(evaluation_result)
        
        return {
            'overall_analysis': 'í‰ê°€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ìƒì„±ëœ í”¼ë“œë°±ì…ë‹ˆë‹¤.',
            'metric_improvements': self._generate_metric_improvements(metrics),
            'improved_prompt': '',
            'error': 'LLM í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ í”¼ë“œë°± ì œê³µ'
        }
    
    def format_feedback(self, feedback: Dict[str, Any]) -> str:
        """í”¼ë“œë°±ì„ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
        
        output = []
        output.append("ğŸ“Š í”„ë¡¬í”„íŠ¸ í‰ê°€ í”¼ë“œë°±")
        output.append("")
        
        # ì „ì²´ ë¶„ì„
        if feedback.get('overall_analysis'):
            output.append("ğŸ” ì „ì²´ ë¶„ì„")
            output.append(feedback['overall_analysis'])
            output.append("")
        
        # ì§€í‘œë³„ ê°œì„  ì—¬ì§€
        if feedback.get('metric_improvements'):
            output.append("ğŸ“ˆ ì§€í‘œë³„ ê°œì„  ì—¬ì§€:")
            for improvement in feedback['metric_improvements']:
                output.append(f"- {improvement}")
            output.append("")
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì œì•ˆ
        if feedback.get('improved_prompt'):
            output.append("âœ¨ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì œì•ˆ:")
            output.append(feedback['improved_prompt'])
        
        return '\n'.join(output)
